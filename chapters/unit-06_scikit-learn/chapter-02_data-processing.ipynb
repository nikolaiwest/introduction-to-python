{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 2:** Data Processing\n",
    "\n",
    "**Data preprocessing** is a critical step in the machine learning pipeline. It involves transforming raw data into a format that is more suitable for modeling. This chapter will delve into essential preprocessing techniques using scikit-learn.\n",
    "\n",
    "<img src=https://i.stack.imgur.com/jEULG.png height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.1 Introduction to the datset](#14-iris-dataset)\n",
    "\n",
    "The *Iris dataset* is one of the most well-known datasets in the field of machine learning and statistics. It's often used as a simple, accessible example for teaching data visualization, machine learning, and data preprocessing techniques. The [dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) was introduced by the British biologist [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) in 1936 as an example of discriminant analysis.\n",
    "\n",
    "```python \n",
    "# Load the data from sklearn\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "```\n",
    "\n",
    "When you load the Iris dataset using `load_iris()` from `sklearn.datasets`, you receive a dataset that consists of `150` samples of iris flowers. These samples are divided evenly across three different species of iris (`50` samples from each of `Setosa`, `Versicolor`, and `Virginica`). \n",
    "\n",
    "<img src=https://miro.medium.com/v2/resize:fit:1400/1*ZK9_HrpP_lhSzTq9xVJUQw.png height=200>\n",
    "\n",
    "For each sample, the dataset includes four features (or `measurements`):\n",
    "\n",
    "1. **Sepal Length:** The length of the sepal (the part of the flower that encases the bud before it blooms) in centimeters.\n",
    "2. **Sepal Width:** The width of the sepal in centimeters.\n",
    "3. **Petal Length:** The length of the petal (the colorful, often bright part of the flower) in centimeters.\n",
    "4. **Petal Width:** The width of the petal in centimeters.\n",
    "\n",
    "These measurements make up the feature matrix `X`, which is a `150x4` `np.array` where each row corresponds to a flower sample, and each column corresponds to one of the four features mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "x_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "\n",
    "print(f\"{x_iris.shape = }\")\n",
    "print(f\"{y_iris.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable `y` is a 1-dimensional array of size `150`, where each entry is an integer representing the species of the corresponding flower in the feature matrix. The species are encoded as `0`, `1`, and `2`, which correspond to `Setosa`, `Versicolor`, and `Virginica`, respectively. \n",
    "\n",
    "The primary use of the Iris dataset in machine learning is for classification tasks, where the goal is to predict the species of an iris flower based on the measurements of its petals and sepals. Because of its simplicity and small size, it's particularly useful for demonstrating the basics of machine learning techniques, from data preprocessing to building various types of models, such as decision trees, support vector machines, or neural networks. \n",
    "\n",
    "In this course, we will use `iris` to introduce many tools from `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.2 Data preprocessing with sklearn](#22-data-preprocessing-with-sklearn)\n",
    "\n",
    "Data preprocessing is an essential stage in the machine learning pipeline, involving several crucial steps to make raw data more suitable for building models. In this chapter, we explore these steps using scikit-learn. \n",
    "\n",
    "Please not that sklearn just provides *one* way to solve these challenges. You can often also use `numpy` or `pandas` to solve these tasks in a similar fashion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values with `SimpleImputer` \n",
    "\n",
    "Missing values in a dataset can lead to inaccurate models. **Scikit-learn**'s `SimpleImputer` offers a convenient way of dealing with this issue by allowing us to replace missing values with a specified placeholder. \n",
    "\n",
    "For example, we can replace missing numerical values with the mean of the remaining non-missing values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Example dataset with missing values\n",
    "x = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "# Using SimpleImputer to replace missing values with the mean of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(x)\n",
    "\n",
    "X_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iris:** While the Iris dataset doesn't have missing values, let's simulate this scenario to demonstrate handling them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Simulate missing values in the first feature\n",
    "x_missing = x_iris.copy()\n",
    "x_missing[::10, 0] = np.nan\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_imputed = imputer.fit_transform(x_missing)\n",
    "\n",
    "x_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling via `StandardScaler`\n",
    "\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled to a standard range. `StandardScaler` is a tool in scikit-learn that standardizes features by removing the mean and scaling to unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dataset\n",
    "x = [[0, 15], [1, -10], [2, 20]]\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iris:** Feature scaling can be demonstrated by standardizing the Iris dataset's features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_iris)\n",
    "\n",
    "# x_scaled now has each feature standardized\n",
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Features with Normalizer \n",
    "\n",
    "Normalization adjusts the scale of data attributes. Scikit-learn's `Normalizer` scales individual samples to have unit norm, a process that can be particularly useful for sparse datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Dataset\n",
    "x = [[4, 1, 2, 2], [1, 3, 9, 3], [5, 7, 5, 1]]\n",
    "\n",
    "# Normalizing the features\n",
    "normalizer = Normalizer()\n",
    "x_normalized = normalizer.fit_transform(x)\n",
    "\n",
    "x_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iris:** Normalization ensures that each sample's feature vector has a unit norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "x_normalized = normalizer.fit_transform(x_iris)\n",
    "\n",
    "# x_normalized now ensures each sample has a unit norm\n",
    "x_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Variables Using `OneHotEncoder`\n",
    "\n",
    "Categorical variables are often represented as `strings` or `numbers` that indicate categories. To make these variables understandable to machine learning algorithms, we use *one-hot encoding*, which represents each category as a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Dataset with categorical features\n",
    "x = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
    "\n",
    "# Applying one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "x_encoded = encoder.fit_transform(x).toarray()\n",
    "\n",
    "x_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iris:**: The Iris dataset's target variable (`y`) is already numerical and represents classes as `0`, `1`, and `2`. Typically, you would **not** one-hot encode the target variable for most scikit-learn classifiers since they handle numerical class labels directly. \n",
    "\n",
    "However, for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y_encoded = encoder.fit_transform(y_iris.reshape(-1, 1)).toarray()\n",
    "\n",
    "# y_encoded is now one-hot encoded\n",
    "y_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection with `SelectKBest`\n",
    "\n",
    "Not all features are equally important for a model. `SelectKBest` allows us to select a subset of the most important features according to a statistical measure, like the [chi-squared](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) test, enhancing model performance and reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Sample dataset\n",
    "x = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "y = [0, 1, 0]\n",
    "\n",
    "# Selecting the 2 best features based on chi-squared test\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "x_selected = selector.fit_transform(x, y)\n",
    "\n",
    "x_selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iris:** To select the two best features according to their [ANOVA F-value](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=2)\n",
    "x_selected = selector.fit_transform(x_iris, y_iris)\n",
    "\n",
    "# x_selected now contains only the 2 features deemed most important\n",
    "x_selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Techniques\n",
    "\n",
    "[Feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html) is crucial for converting raw data into a structured format. Scikit-learn provides several tools for this purpose, such as `DictVectorizer` for feature arrays represented as lists of dictionaries and `CountVectorizer` for converting text documents into a matrix of token counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "corpus = ['text data preprocessing', 'feature extraction with scikit-learn']\n",
    "\n",
    "# Vectorizing text data\n",
    "vectorizer = CountVectorizer()\n",
    "x_vectorized = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "x_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iris:** Feature extraction is more relevant to datasets with raw data like text or images. Since the Iris dataset is already in a structured format, feature extraction like text vectorization doesn't directly apply. However, you can transform or create new features based on existing ones, a process known as feature engineering, which might involve calculating interactions between features or other transformations.\n",
    "\n",
    "In summary, these examples demonstrate how to apply various preprocessing techniques to generic data and to the Iris dataset using scikit-learn, preparing the data for machine learning modeling. \n",
    "\n",
    "This chapter prepares for for the important task of data preprocessing and ensuring that your dataset is in the best possible shape for training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.3 Preparing for model training](#23-preparing-for-model-training)\n",
    "\n",
    "Before training a model, it's vital to prepare your dataset properly. Besides data preprocessing, this preparation involves creating separate datasets for training and testing. By doing so, you can train your model on one portion of the data and evaluate its performance on a separate set that it hasn't seen before, which is crucial for assessing the model's ability to generalize.\n",
    "\n",
    "<img src=https://miro.medium.com/v2/resize:fit:580/1*OECM6SWmlhVzebmSuvMtBg.png height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting a dataset using `train_test_split`\n",
    "\n",
    "The purpose of splitting your dataset into training and testing sets is to provide an honest assessment of the model's performance. Scikit-learn offers the `train_test_split function`, which is an efficient way to randomly partition the dataset.\n",
    "\n",
    "The `train_test_split` function shuffles your data and then splits it into training and testing subsets. You can specify the proportion of the dataset you want to allocate for testing with the test_size parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate dummy data\n",
    "x_dummy = np.random.rand(100, 4)  # 100 samples, 4 features\n",
    "y_dummy = np.random.randint(0, 2, 100)  # Binary target variable\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "x_train_dummy, x_test_dummy, y_train_dummy, y_test_dummy = train_test_split(x_dummy, y_dummy, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {x_train_dummy.shape[0]} samples\")\n",
    "print(f\"Testing set size: {x_test_dummy.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to apply it to the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming x and y are already defined (Iris dataset features and target)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "# This splits the dataset into 80% training data and 20% testing data\n",
    "print(f\"{x_train.shape = }\")\n",
    "print(f\"{x_test.shape = }\")\n",
    "print(f\"{y_train.shape = }\")\n",
    "print(f\"{y_test.shape = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `20%` of the data is reserved for testing, and the `random_state` parameter ensures that the split is reproducible; using the same seed will produce the same split in future runs.\n",
    "\n",
    "#### Importance of Random State in Reproducibility\n",
    "\n",
    "In some cases, especially in datasets where some classes are underrepresented, it's important to use a stratified split. This method ensures that the proportion of classes in both the training and testing sets reflects that of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the total number of classes in the y values of our iris data\n",
    "print(f\"{np.unique(y_train, return_counts=True)}\")\n",
    "print(f\"{np.unique(y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn's `train_test_split` function allows for *stratified* splitting by using the `stratify` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_iris, y_iris, test_size=0.2, stratify=y_iris, random_state=41)\n",
    "\n",
    "# This again splits the dataset into 80% training data and 20% testing data, this time accounting for the y ratio\n",
    "# Lets check the total number of classes in the y values of our iris data\n",
    "print(f\"{np.unique(y_train, return_counts=True)}\")\n",
    "print(f\"{np.unique(y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `stratify=y_iris`, we ensure that the distribution of classes in both the training and testing sets matches the original dataset as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation: An Alternative to Train/Test Split\n",
    "\n",
    "While splitting your data into `training` and `testing` sets is a good practice, it's also useful to employ `cross-validation` as an additional step. \n",
    "\n",
    "<img src=https://www.researchgate.net/publication/340567535/figure/fig2/AS:880966289588226@1587050139118/Train-test-cross-validation-split-methodology-used-in-this-paper-The-first-operation.jpg height= 400>\n",
    "\n",
    "Cross-validation involves splitting the dataset into multiple chunks (called a `fold`) and training multiple models by using different chunks as the test set each time. This process helps in assessing the model's performance more robustly.\n",
    "\n",
    "Scikit-learn offers various [cross-validation strategies](https://scikit-learn.org/stable/modules/cross_validation.html), but a simple and commonly used method is **K-fold cross-validation**, which can be performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a simple classifier (We will cover classifier in detail in chapter 3.1)\n",
    "classifier_dummy = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "scores_dummy = cross_val_score(classifier_dummy, x_dummy, y_dummy, cv=5)\n",
    "\n",
    "print(\"Cross-validation accuracy scores:\", scores_dummy) # (We will cover scoring in chapter 3.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to apply cross validation to our iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(classifier, x_iris, y_iris, cv=5)\n",
    "\n",
    "print(\"Accuracy scores for each fold:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the dataset is split into five parts; the model is trained five times, each time using a different part as the test set. The `cross_val_score` function then returns the accuracy of the model for each fold, providing insight into how the model performs across different subsets of the data.\n",
    "\n",
    "By combining train/test splits with cross-validation, you can ensure that your models are both effective and generalizable, ready for the final steps of training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# --> ðŸš€ðŸ’»ðŸ’¥ *Coding Challenge ([Step 0-3]((#34-coding-challenge)))*\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
