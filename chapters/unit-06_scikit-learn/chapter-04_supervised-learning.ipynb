{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chapter 3:** Supervised learning\n",
    "\n",
    "**Supervised learning** is a cornerstone of machine learning, where the goal is to learn a mapping from inputs (`features`) to outputs (`targets`), given a labeled dataset. This chapter delves into the two primary categories of supervised learning: *classification* and *regression*. Using `scikit-learn`, we'll explore how to apply these concepts to real-world datasets, including a detailed walkthrough with the Iris dataset for classification and another dataset for regression to provide comprehensive insights.\n",
    "\n",
    "Scikit-learn offers a comprehensive suite of tools for implementing supervised learning models, which you can find explained in detail for [regression](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) and for [classification](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.1 Classification models](#22-classification-models)\n",
    "\n",
    "The Iris dataset is *the* classic example for classification, where the task is to predict the species of an iris flower based on its sepal and petal dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.2 Regression models](#21-regression-models):\n",
    "\n",
    "In `regression models`, the goal is to predict a continuous value. Scikit-learn provides several regression models, including linear regression, decision trees, and support vector regression. For example, predicting the price of a house based on its features (size, location, etc.) is a regression problem.\n",
    "\n",
    "#### Boston housing dataset for regression \n",
    "\n",
    "Since we cannot use iris for regression, let's use the [California Housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), which is a popular dataset for regression tasks. \n",
    "\n",
    "This dataset contains information related to housing in California, such as `median income`, `housing median age`, `average rooms`, `average bedrooms`, `population`, `average occupancy`, `latitude`, and `longitude`. The target variable is the `median house value` for California districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "x_h, y_h = housing.data, housing.target\n",
    "\n",
    "print(f\"{x_h.shape = }\")\n",
    "print(f\"{y_h.shape = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we also included data scaling using `StandardScaler` before splitting the data. As introduced in the last chapter, scaling features is a common preprocessing step for many machine learning algorithms, especially for those sensitive to the scale of the data, like SVMs or neural networks. \n",
    "\n",
    "However, tree-based models like *RandomForest* are generally scale-invariant but applying scaling can still be beneficial for convergence speed in some cases or when using regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We follow the good practice and scale the data for regression tasks\n",
    "scaler = StandardScaler()\n",
    "x_h_scaled = scaler.fit_transform(x_h)\n",
    "\n",
    "# Next, we split the dataset into training and testing sets\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(x_h_scaled, y_h, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "regressor_h = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "regressor_h.fit(X_train_h, y_train_h)\n",
    "\n",
    "# Predict the median house values on the testing set\n",
    "y_pred_h = regressor_h.predict(X_test_h)\n",
    "\n",
    "# Evaluate the model using the mean squared error (MSE) metric\n",
    "mse_h = mean_squared_error(y_test_h, y_pred_h)\n",
    "print(f\"Regression Mean Squared Error on California Housing Dataset: {mse_h:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comprehensive approach, from data loading and preprocessing to model training and evaluation, illustrates the typical workflow for a regression task in supervised learning. \n",
    "\n",
    "By exploring different datasets and regression models, you gain a deeper understanding of how to tackle various types of regression problems effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.3 Model Evaluation](#33-model-evaluation)\n",
    "\n",
    "Model evaluation is a critical step in the machine learning workflow. It allows you to assess the performance of your model and understand its strengths and weaknesses. In this chapter, weâ€™ll cover essential evaluation metrics and tools used in both classification and regression tasks. Understanding these concepts will help you choose the right metric for your specific problem and ensure that your model meets the desired objectives.\n",
    "\n",
    "#### Classification Metrics\n",
    "\n",
    "For classification problems, *accuracy*, *precision*, *recall*, *F1 score*, and the *confusion matrix* are commonly used metrics.\n",
    "\n",
    "* **Accuracy:** Accuracy measures the proportion of true results (both true positives and true negatives) among the total number of cases examined. It's a good measure when the class distributions are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = [0,0,1,0,0,1,0,0,1,0]\n",
    "y_pred = [0,1,0,0,0,1,1,0,1,0]\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted labels respectively\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Precision and Recall:** Precision measures the accuracy of positive predictions (i.e., the proportion of true positives among all positive predictions), whereas recall (or sensitivity) measures the ability of the classifier to find all positive samples (i.e., the proportion of true positives among all actual positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **F1-Score:** The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. It's especially useful when the class distribution is uneven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Confusion Matrix:** The confusion matrix is a table that describes the performance of a classification model on a set of test data for which the true values are known. It allows you to see the errors made by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Metrics\n",
    "\n",
    "For regression tasks, *Mean Squared Error (MSE)* and *Mean Absolute Error (MAE)* are widely used metrics.\n",
    "\n",
    "* **Mean Squared Error (MSE):** MSE measures the average squared difference between the estimated values and the actual value. It gives a rough idea of the magnitude of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = [1.1,2,3.3,4,5]\n",
    "y_pred = [2.2,2,3,5,6]\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted values respectively\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Mean Absolute Error (MAE):** MAE measures the average absolute difference between the estimated values and the actual value, providing a linear score that weights all errors equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the right metric\n",
    "\n",
    "The choice of metric depends on your specific problem and goals. For instance, in a medical diagnosis problem, recall might be more important than precision, as missing a positive case could be more detrimental than falsely labeling a negative case as positive. Conversely, in a spam detection system, precision might be more critical to avoid filtering out important emails.\n",
    "\n",
    "Understanding these metrics and their implications will help you evaluate your models effectively, guiding you towards making improvements and ultimately achieving better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# --> ðŸš€ðŸ’»ðŸ’¥ *Coding Challenge ([Step 4-5]((#34-coding-challenge)))*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3.4 Coding Challenge](#34-coding-challenge)\n",
    "\n",
    "As a practical exercise, we revisit the injection modling dataset used in the matplotlib course unit. This challenge will involve predicting the `\"quality\"` column from the provided dataset. \n",
    "\n",
    "Like before, we'll break down the challenge into several steps, each focusing on a different aspect of the machine learning process.  \n",
    "\n",
    "#### Task \n",
    "\n",
    "Develop a machine learning model to predict the `\"quality\"` of our injection modling experiments based on the various manufacturing parameters using scikit-learn.\n",
    "\n",
    "#### The dataset \n",
    "\n",
    "The dataset should be already known form the previous exercise. It includes manufacturing parameters like `melt temperature`, `mold temperature`, and various measurements related to the manufacturing process, with the target variable being `\"quality\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0:** Load and Examine the Dataset\n",
    "\n",
    "* Load the `data.csv` dataset, encompassing process parameters and each lens's quality classification.\n",
    "* Conduct an initial examination to comprehend its composition and content structure (e.g. by printing the head of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the injection molding data like we did in the last course unit\n",
    "import pandas as  pd \n",
    "\n",
    "data = pd.read_csv(\"../data/data.csv\", delimiter=\";\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Data Loading and Preprocessing\n",
    "* Separate the measurements from the target variable (`\"quality\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the measurements from the target variable \"quality\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "y = data[\"quality\"]\n",
    "x = data[data.columns[:-1]]\n",
    "# x = data.drop(\"quality\", axis=1)\n",
    "\n",
    "print(f\"{x.shape = }\")\n",
    "print(f\"{y.shape = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Splitting the Data\n",
    "* Split the dataset into training and testing sets using `train_test_split`.\n",
    "\n",
    "* Select a `test_size` of `20%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets using `train_test_split`.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"{x_train.shape = }\")\n",
    "print(f\"{x_test.shape = }\")\n",
    "print(f\"{y_train.shape = }\")\n",
    "print(f\"{y_test.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Feature Scaling\n",
    "* Apply feature scaling to the dataset using the `StandardScaler` to standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling to the dataset using the StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Model Selection and Training\n",
    "\n",
    "* Choose a suitable model. For this example, we can again use `RandomForestClassifier` as a starting point.\n",
    "\n",
    "* Train the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training set\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "scores_cv = cross_val_score(model, x_test_scaled, y_test, cv=5)\n",
    "\n",
    "print(\"Cross-validation accuracy scores:\", scores_cv) # (We will cover scoring in chapter 3.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Model Evaluation\n",
    "* Evaluate the model's performance on the test set using appropriate metrics.\n",
    "\n",
    "    * We are going to apply `accuracy`, `precision`, `recall` and `f1 score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test set using appropriate metrics.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge provides a comprehensive overview of applying supervised learning techniques to a real-world dataset, from preprocessing and model training to evaluation and optimization, offering a hands-on experience with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[--> Back to Outline](#course-outline)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
